{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "realistic-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import Row\n",
    "from google.cloud import storage\n",
    "from datetime import time, datetime\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabulous-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOMBRES VARIABLES PARA TABLA TEMPORAL\n",
    "param_str_afi = \"4040\"\n",
    "anio_mes_dia_d = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "anio_mes_dia_t = datetime.now().strftime(\"%Y_%m_%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "featured-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "project_id = \"dfa-dna-ws0009-la-prd-d0bb\"\n",
    "dataset_id = \"dfa_dna_ws0009_la_prd_sandbox\"\n",
    "dataset_landing_id = \"dfa-dna-ws0009-la-prd-landing-zone\"\n",
    "location = \"northamerica-northeast1\"\n",
    "\n",
    "#NOMBRES CSV DE SALIDA\n",
    "input_filename = \"IP_INPUT.csv\"\n",
    "tmp_filename = \"IP_TMP.csv\"\n",
    "output_filename = f\"IP_OUTPUT_{param_str_afi}_{anio_mes_dia_t}\"\n",
    "\n",
    "#NOMBRE DE CARPETAS\n",
    "input_folder = f\"income_predictor/in/\"\n",
    "tmp_folder = f\"income_predictor/tmp/\"\n",
    "out_folder = f\"income_predictor/out/\"\n",
    "\n",
    "#TABLES NAMES\n",
    "table_name_query = \"uy_efx_score_ip_monthly_hash_la_prd\"\n",
    "table_name_temp_sandbook_1 = f\"tmp_input_cliente_ip_{param_str_afi}\"\n",
    "table_name_temp_sandbook_2 = f\"tmp_input_cliente_ip_{param_str_afi}_{anio_mes_dia_t}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stone-boards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 599 rows.\n"
     ]
    }
   ],
   "source": [
    "#TRAER CSV IN\n",
    "storage_client = storage.Client()\n",
    "gcs_bucket = dataset_landing_id\n",
    "\n",
    "#CREAR TABLA TEMP\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"ID\", \"STRING\", mode = \"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"DOCUMENTO\", \"STRING\", mode = \"NULLABLE\")\n",
    "    ]\n",
    "\n",
    "#TABLA NAME\n",
    "client = bigquery.Client()\n",
    "tmp_table_name = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_1\n",
    "table = bigquery.Table(tmp_table_name, schema=schema)\n",
    "table = client.create_table(table)\n",
    "\n",
    "#CARGAR CSV AL SCHEMA\n",
    "job_config = bigquery.LoadJobConfig(schema = schema, skip_leading_rows = 1, source_format = bigquery.SourceFormat.CSV, field_delimiter=\";\")\n",
    "uri = f\"gs://{gcs_bucket}/{input_folder + input_filename}\"\n",
    "load_job = client.load_table_from_uri(uri, tmp_table_name, job_config = job_config)  \n",
    "load_job.result()  \n",
    "\n",
    "destination_table = client.get_table(tmp_table_name)  \n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fewer-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET TABLE\n",
    "client = bigquery.Client()\n",
    "table_id_1 = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_1\n",
    "table_1 = client.get_table(table_id_1)\n",
    "\n",
    "client = bigquery.Client()\n",
    "table_id_2 = project_id + \".\" + dataset_id + \".\" + table_name_query\n",
    "table_2 = client.get_table(table_id_2)\n",
    "\n",
    "sql = f\"SELECT  T1.* \\\n",
    "            , T2.cat_ip \\\n",
    "FROM (SELECT documento, ID as id_input FROM {table_id_1}) T1 left join \\\n",
    "        (SELECT documento AS documento, cast(cat_ip as integer) as CAT_IP \\\n",
    "         FROM {table_id_2}) T2 \\\n",
    "ON T1.documento = T2.documento\".format(param_str_afi)\n",
    "\n",
    "#CREAR DATAFRAME\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "#SETUP UPLOAD\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(dataset_landing_id)\n",
    "\n",
    "#UPLOAD CSV TO BUCKET TEMP\n",
    "bucket.blob(input_folder + input_filename).upload_from_string(df.to_csv(index = False, sep = ','), 'text/csv')\n",
    "\n",
    "##traer de un bucket\n",
    "storage_client = storage.Client()\n",
    "gcs_bucket = dataset_landing_id\n",
    "\n",
    "bucket = storage_client.get_bucket(gcs_bucket)\n",
    "blob = bucket.get_blob(input_folder + input_filename)\n",
    "downloaded_blob = blob.download_as_string()\n",
    "\n",
    "#transformar data\n",
    "decoded_csv = downloaded_blob.decode('utf-8').splitlines()\n",
    "reader = csv.reader(decoded_csv, delimiter = \";\")\n",
    "parsed_csv = list(reader)\n",
    "\n",
    "for row in parsed_csv:\n",
    "    if row[0] == \"documento\":\n",
    "        print(f\"Header: {','.join(row)}\")\n",
    "        row.append('param_str_afi,anio_mes_dia_d')\n",
    "    else:\n",
    "        row.append(param_str_afi)\n",
    "        row.append(anio_mes_dia_d)\n",
    "lista_records = []\n",
    "\n",
    "for row in parsed_csv:\n",
    "    lista_records.append(\",\".join(row))\n",
    "\n",
    "csv_string = \"\\n\".join(lista_records)\n",
    "\n",
    "#Guardar CSV enriquecido\n",
    "blob = bucket.blob(tmp_folder + tmp_filename)\n",
    "blob.upload_from_string(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "important-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 599 rows.\n"
     ]
    }
   ],
   "source": [
    "#TRAER CSV IN\n",
    "storage_client = storage.Client()\n",
    "gcs_bucket = dataset_landing_id\n",
    "\n",
    "#CREAR TABLA TEMP\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"documento\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"id_input\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cat_ip\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"param_str_afi\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"anio_mes_dia\", \"DATE\", mode=\"NULLABLE\")\n",
    "    ]\n",
    "\n",
    "#TABLA NAME\n",
    "client = bigquery.Client()\n",
    "tmp_table_name = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_2\n",
    "table = bigquery.Table(tmp_table_name, schema=schema)\n",
    "table = client.create_table(table)\n",
    "\n",
    "#CARGAR CSV AL SCHEMA\n",
    "job_config = bigquery.LoadJobConfig(schema = schema, skip_leading_rows = 1, source_format = bigquery.SourceFormat.CSV)\n",
    "uri = f\"gs://{gcs_bucket}/{tmp_folder + tmp_filename}\"\n",
    "load_job = client.load_table_from_uri(uri, tmp_table_name, job_config=job_config)  \n",
    "load_job.result()  \n",
    "\n",
    "destination_table = client.get_table(tmp_table_name)  \n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "#GET TABLE\n",
    "client = bigquery.Client()\n",
    "table_id_1 = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_2\n",
    "table_1 = client.get_table(table_id_1)\n",
    "\n",
    "sql = f\"SELECT * FROM {table_id_1} \\\n",
    "        ORDER BY id_input\"\n",
    "\n",
    "#CREAR DATAFRAME\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "#SETUP UPLOAD\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(dataset_landing_id)\n",
    "\n",
    "#UPLOAD CSV TO BUCKET TEMP\n",
    "bucket.blob(out_folder + output_filename).upload_from_string(df.to_csv(index = False, sep = ','), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bibliographic-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET TABLE\n",
    "client = bigquery.Client()\n",
    "table_id_1 = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_2\n",
    "table_1 = client.get_table(table_id_1)\n",
    "\n",
    "sql = f\"SELECT * FROM {table_id_1} \\\n",
    "        ORDER BY id_input\"\n",
    "\n",
    "#CREAR DATAFRAME\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "#SETUP UPLOAD\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(dataset_landing_id)\n",
    "\n",
    "#UPLOAD CSV TO BUCKET TEMP\n",
    "bucket.blob(out_folder + output_filename).upload_from_string(df.to_csv(index = False, sep = ','), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "green-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted csv '<Blob: dfa-dna-ws0009-la-prd-landing-zone, income_predictor/in/IP_INPUT.csv, 1658434821289656>'.\n",
      "Deleted csv '<Blob: dfa-dna-ws0009-la-prd-landing-zone, income_predictor/tmp/IP_TMP.csv, 1658434821502655>'.\n"
     ]
    }
   ],
   "source": [
    "#DELETE IN CSV\n",
    "bucket1 = storage_client.get_bucket(gcs_bucket)\n",
    "blob1 = bucket1.get_blob(input_folder + input_filename)\n",
    "blob1.delete()\n",
    "\n",
    "#DELETE TEMP CSV\n",
    "bucket = storage_client.get_bucket(gcs_bucket)\n",
    "blobt = bucket.get_blob(tmp_folder + tmp_filename)\n",
    "blobt.delete()\n",
    "\n",
    "print(\"Deleted csv '{}'.\".format(blob1))\n",
    "print(\"Deleted csv '{}'.\".format(blobt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "champion-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table 'dfa-dna-ws0009-la-prd-d0bb.dfa_dna_ws0009_la_prd_sandbox.tmp_input_cliente_ip_4040_2022_07_21'.\n",
      "Deleted table 'dfa-dna-ws0009-la-prd-d0bb.dfa_dna_ws0009_la_prd_sandbox.uy_efx_score_ip_monthly_hash_la_prd'.\n",
      "Proceso finalizado con exito, revisar csv carpeta income_predictor/out\n"
     ]
    }
   ],
   "source": [
    "#BORRAR TABLA EN SANDBOOK\n",
    "#APLICAR DESPUES DE OCUPAR LA TABLA\n",
    "client = bigquery.Client()\n",
    "table_id_1 = (project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_1)\n",
    "\n",
    "client = bigquery.Client()\n",
    "table_id_1 = (project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_2)\n",
    "\n",
    "client.delete_table(table_id_1, not_found_ok = True)\n",
    "client.delete_table(table_id_2, not_found_ok = True)\n",
    "print(\"Deleted table '{}'.\".format(table_id_1))\n",
    "print(\"Deleted table '{}'.\".format(table_id_2))\n",
    "\n",
    "print(\"Proceso finalizado con exito, revisar csv carpeta income_predictor/out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf24faee159193869fbc6da8f91bdaa5a36ee21dfa23a6992a5b303848d26b48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
