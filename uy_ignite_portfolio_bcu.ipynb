{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secondary-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import Row\n",
    "from google.cloud import storage\n",
    "from datetime import time, datetime\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "early-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOMBRES VARIABLES PARA TABLA TEMPORAL\n",
    "par_db = \"4040\"\n",
    "par_periodo_d = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "par_periodo_t = datetime.now().strftime(\"%Y_%m_%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "finished-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "project_id = \"dfa-dna-ws0009-la-prd-d0bb\"\n",
    "dataset_id = \"dfa_dna_ws0009_la_prd_sandbox\"\n",
    "dataset_landing_id = \"dfa-dna-ws0009-la-prd-landing-zone\"\n",
    "location = \"northamerica-northeast1\"\n",
    "\n",
    "#NOMBRES CSV DE SALIDA\n",
    "input_filename = \"t1_in.csv\"\n",
    "tmp_filename = \"ignite_portfolio_bcu_tmp.csv\"\n",
    "output_filename = f\"ignite_portfolio_bcu_out_{par_db}_{par_periodo_t}\"\n",
    "\n",
    "#NOMBRE DE CARPETAS\n",
    "input_folder = f\"ignite_portfolio_bcu/in/\"\n",
    "tmp_folder = f\"ignite_portfolio_bcu/tmp/\"\n",
    "out_folder = f\"ignite_portfolio_bcu/out/\"\n",
    "\n",
    "#TABLES NAMES\n",
    "table_name_query_1 =  \"uy_efx_bcu_hash_la_prd\"\n",
    "table_name_query_2 =  \"uy_efx_credito_weekly_la_prd\"\n",
    "table_name_temp_sandbook = f\"tmp_input_cliente_ignite_portfolio_bcu_{par_db}_{par_periodo_t}\"\n",
    "table_name_temp_sandbook_s = f\"tmp_input_cliente_ignite_portfolio_bcu_soporte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "maritime-silence",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1598 rows.\n"
     ]
    }
   ],
   "source": [
    "#CREAR TABLA SOPRTE\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"ID\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"documento\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"MARCA_FINAL\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"TIPO_FICHA\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ARCHIVE\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "#TABLA NAME\n",
    "client = bigquery.Client()\n",
    "tmp_table_name = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_s\n",
    "table = bigquery.Table(tmp_table_name, schema=schema)\n",
    "table = client.create_table(table)\n",
    "\n",
    "#TRAER UN BUCKET\n",
    "storage_client = storage.Client()\n",
    "gcs_bucket = dataset_landing_id\n",
    "\n",
    "#CREAR SCHEMA\n",
    "bucket = storage_client.get_bucket(gcs_bucket)\n",
    "blob = bucket.get_blob(input_folder + input_filename)\n",
    "downloaded_blob = blob.download_as_string()\n",
    "\n",
    "#CARGAR CSV AL SCHEMA\n",
    "gcs_bucket = dataset_landing_id\n",
    "job_config = bigquery.LoadJobConfig(schema = schema, skip_leading_rows = 1, source_format = bigquery.SourceFormat.CSV, field_delimiter=\",\")\n",
    "uri = f\"gs://{gcs_bucket}/{input_folder + input_filename}\"\n",
    "load_job = client.load_table_from_uri(uri, tmp_table_name, job_config = job_config)  \n",
    "load_job.result()  \n",
    "\n",
    "#VERIFICACION\n",
    "destination_table = client.get_table(tmp_table_name)  \n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smoking-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLAMAR TABLAS\n",
    "client = bigquery.Client()\n",
    "table_id_1 = project_id + \".\" + dataset_id + \".\" + table_name_query_1\n",
    "table_1 = client.get_table(table_id_1)\n",
    "\n",
    "client = bigquery.Client()\n",
    "table_id_2 = project_id + \".\" + dataset_id + \".\" + table_name_query_2\n",
    "table_2 = client.get_table(table_id_2)\n",
    "\n",
    "client = bigquery.Client()\n",
    "table_id_3 = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_s\n",
    "table_3 = client.get_table(table_id_3)\n",
    "\n",
    "#QUERY IGNITE BCU\n",
    "sql = f\"SELECT  T1.documento \\\n",
    "        , T1.documento AS RUT \\\n",
    "        , T4.NOMBRE AS RAZON_SOCIAL \\\n",
    "        , T2.entidad \\\n",
    "        , T2.calificacion as calificacion_entidad \\\n",
    "        , TRUNC(t2.SALDO_VIGENTE_MN) AS SALDO_VIGENTE_MN \\\n",
    "        , TRUNC(t2.SALDO_VENCIDO_MN) AS SALDO_VENCIDO_MN \\\n",
    "        , TRUNC((t2.SALDO_VIGENTE_MN+t2.SALDO_VENCIDO_MN)) AS SALDO_MN \\\n",
    "        , TRUNC(t2.SALDO_VIGENTE_ME) AS SALDO_VIGENTE_ME \\\n",
    "        , TRUNC(t2.SALDO_VENCIDO_ME) AS SALDO_VENCIDO_ME \\\n",
    "        , TRUNC((t2.SALDO_VIGENTE_ME+t2.SALDO_VENCIDO_ME)) AS SALDO_ME \\\n",
    "        , T3.PEOR_BCU_TOTAL \\\n",
    "        , 1 as ACTIVIDAD \\\n",
    "        , 0 AS FALLECIDO \\\n",
    "        , T1.TIPO_FICHA \\\n",
    "FROM \\\n",
    "   `\" + table_id_3 + \"` T1 JOIN \\\n",
    "    ( \\\n",
    "    SELECT  T1.PERS_ID \\\n",
    "            , UPPER(T2.ENTIDAD.NOMBRE) AS ENTIDAD \\\n",
    "            , T2.CALIFICACION \\\n",
    "            , SUM(CASE WHEN T3.CODIGO = 12410009  AND T3.MONEDA = 'MN' THEN T3.IMPORTE ELSE 0 END) AS SALDO_VIGENTE_MN \\\n",
    "            , SUM(CASE WHEN T3.CODIGO = 12600006  AND T3.MONEDA = 'MN' THEN T3.IMPORTE ELSE 0 END) AS SALDO_VENCIDO_MN \\\n",
    "            \\\n",
    "            , SUM(CASE WHEN T3.CODIGO = 12410009  AND T3.MONEDA = 'ME' THEN T3.IMPORTE ELSE 0 END) AS SALDO_VIGENTE_ME \\\n",
    "            , SUM(CASE WHEN T3.CODIGO = 12600006  AND T3.MONEDA = 'ME' THEN T3.IMPORTE ELSE 0 END) AS SALDO_VENCIDO_ME \\\n",
    "    FROM `\" + table_id_1 + \"` T1, \\\n",
    "            T1.DEUDAS T2, \\\n",
    "            T2.RUBROS T3 \\\n",
    "    WHERE T1.ARCHIVE = '2022-01-01' \\\n",
    "            AND T3.IMPORTE IS NOT NULL \\\n",
    "    group by T1.PERS_ID, t2.calificacion, T2.ENTIDAD.NOMBRE \\\n",
    "    ) T2 ON T1.documento = T2.PERS_ID  JOIN \\\n",
    "    ( \\\n",
    "    SELECT  T1.PERS_ID \\\n",
    "            , MAX(T2.CALIFICACION) AS PEOR_BCU_TOTAL \\\n",
    "    FROM `\" + table_id_1 + \"` T1, \\\n",
    "            T1.DEUDAS T2 \\\n",
    "    WHERE  T1.ARCHIVE = '2022-01-01' \\\n",
    "    group by T1.PERS_ID \\\n",
    "    ) T3 ON T1.documento = T3.PERS_ID \\\n",
    "    LEFT JOIN \\\n",
    "    ( \\\n",
    "        SELECT  PERS_ID \\\n",
    "            , TIPO_DOCUMENTO \\\n",
    "            , CASE WHEN T1.TIPO_DOCUMENTO = 'CI' THEN \\\n",
    "                    CONCAT \\\n",
    "                    ( ifnull(T1.APELLIDO_1ERO, '') \\\n",
    "                    , ifnull(T1.APELLIDO_2DO, '') \\\n",
    "                    , ifnull(T1.NOMBRE_1ERO, '') \\\n",
    "                    , ifnull(T1.NOMBRE_2DO, '') \\\n",
    "                    ) \\\n",
    "                    ELSE \\\n",
    "                    ifnull(T1.razon_social, '') \\\n",
    "                    END AS NOMBRE \\\n",
    "                    \\\n",
    "        FROM `\" + table_id_2 + \"` T1 \\\n",
    "    ) T4 ON T1.documento = T4.PERS_ID\"\n",
    "\n",
    "#CREAR DATAFRAME\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "#SETUP UPLOAD\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(dataset_landing_id)\n",
    "\n",
    "#UPLOAD CSV TO BUCKET TEMP\n",
    "bucket.blob(tmp_folder + tmp_filename).upload_from_string(df.to_csv(index = False, sep = ','), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accredited-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAR TABLA TEMP SANDBOOK\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"documento\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"RUT\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"RAZON_SOCIAL\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"entidad\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"calificacion_entidad\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"SALDO_VIGENTE_MN\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"SALDO_VENCIDO_MN\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"SALDO_MN\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"SALDO_VIGENTE_ME\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"SALDO_VENCIDO_ME\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"SALDO_ME\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"PEOR_BCU_TOTAL\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ACTIVIDAD\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"FALLECIDO\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"TIPO_FICHA\", \"STRING\", mode=\"NULLABLE\")\n",
    "]\n",
    "\n",
    "#TABLA NAME\n",
    "client = bigquery.Client()\n",
    "tmp_table_name = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook\n",
    "table = bigquery.Table(tmp_table_name, schema=schema)\n",
    "table = client.create_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nervous-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 rows.\n"
     ]
    }
   ],
   "source": [
    "#TRAER UN BUCKET\n",
    "storage_client = storage.Client()\n",
    "gcs_bucket = dataset_landing_id\n",
    "\n",
    "#CREAR SCHEMA\n",
    "bucket = storage_client.get_bucket(gcs_bucket)\n",
    "blob = bucket.get_blob(tmp_folder + tmp_filename)\n",
    "downloaded_blob = blob.download_as_string()\n",
    "\n",
    "#CARGAR CSV AL SCHEMA\n",
    "gcs_bucket = dataset_landing_id\n",
    "job_config = bigquery.LoadJobConfig(schema = schema, skip_leading_rows = 1, source_format = bigquery.SourceFormat.CSV)\n",
    "uri = f\"gs://{gcs_bucket}/{tmp_folder + tmp_filename}\"\n",
    "load_job = client.load_table_from_uri(uri, tmp_table_name, job_config = job_config)  \n",
    "load_job.result()  \n",
    "\n",
    "#REVISAR\n",
    "destination_table = client.get_table(tmp_table_name)  \n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unlike-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET TABLE\n",
    "client = bigquery.Client()\n",
    "table_id_1 = project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook\n",
    "table_1 = client.get_table(table_id_1)\n",
    "\n",
    "sql = f\"SELECT * FROM {table_id_1}\"\n",
    "\n",
    "#CREAR DATAFRAME\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "#SETUP UPLOAD\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(dataset_landing_id)\n",
    "\n",
    "#UPLOAD CSV TO BUCKET TEMP\n",
    "bucket.blob(out_folder + output_filename).upload_from_string(df.to_csv(index = False, sep = ','), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unexpected-azerbaijan",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted csv '<Blob: dfa-dna-ws0009-la-prd-landing-zone, ignite_portfolio_bcu/in/t1_in.csv, 1658436847779280>'.\n",
      "Deleted csv '<Blob: dfa-dna-ws0009-la-prd-landing-zone, ignite_portfolio_bcu/tmp/ignite_portfolio_bcu_tmp.csv, 1658437019240865>'.\n"
     ]
    }
   ],
   "source": [
    "#DELETE IN CSV\n",
    "gcs_bucket = dataset_landing_id\n",
    "bucket1 = storage_client.get_bucket(gcs_bucket)\n",
    "blob1 = bucket1.get_blob(input_folder + input_filename)\n",
    "blob1.delete()\n",
    "\n",
    "#DELETE TEMP CSV\n",
    "buckett = storage_client.get_bucket(gcs_bucket)\n",
    "blobt = buckett.get_blob(tmp_folder + tmp_filename)\n",
    "blobt.delete()\n",
    "\n",
    "print(\"Deleted csv '{}'.\".format(blob1))\n",
    "print(\"Deleted csv '{}'.\".format(blobt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "individual-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted table 'dfa-dna-ws0009-la-prd-d0bb.dfa_dna_ws0009_la_prd_sandbox.tmp_input_cliente_ignite_portfolio_bcu_4040_2022_07_21'.\n",
      "Deleted table 'dfa-dna-ws0009-la-prd-d0bb.dfa_dna_ws0009_la_prd_sandbox.tmp_input_cliente_ignite_portfolio_bcu_soporte'.\n",
      "Proceso finalizado con exito, revisar csv carpeta ignite_portfolio_bcu/out\n"
     ]
    }
   ],
   "source": [
    "#BORRAR TABLAS EN SANDBOOK\n",
    "#APLICAR DESPUES DE OCUPAR LA TABLA\n",
    "client = bigquery.Client()\n",
    "\n",
    "table_id = (project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook)\n",
    "client.delete_table(table_id, not_found_ok = True)\n",
    "\n",
    "table_id_s = (project_id + \".\" + dataset_id + \".\" + table_name_temp_sandbook_s)\n",
    "client.delete_table(table_id_s, not_found_ok = True)\n",
    "\n",
    "print(\"Deleted table '{}'.\".format(table_id))\n",
    "print(\"Deleted table '{}'.\".format(table_id_s))\n",
    "\n",
    "print(\"Proceso finalizado con exito, revisar csv carpeta ignite_portfolio_bcu/out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
